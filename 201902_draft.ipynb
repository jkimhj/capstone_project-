{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "data = pd.read_csv('bq-results-20190608-151444-lhfwg26di18j.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 7 columns):\n",
      "parent_id       500000 non-null object\n",
      "subreddit_id    500000 non-null object\n",
      "subreddit       500000 non-null object\n",
      "created_utc     500000 non-null int64\n",
      "author          500000 non-null object\n",
      "score           500000 non-null int64\n",
      "body            499996 non-null object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 26.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>t3_aunzqy</td>\n",
       "      <td>t5_x5zpe</td>\n",
       "      <td>MadridBarcelonaLive</td>\n",
       "      <td>1551140268</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>t3_aunzqy</td>\n",
       "      <td>t5_x5zpe</td>\n",
       "      <td>MadridBarcelonaLive</td>\n",
       "      <td>1551140453</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>t3_aunzqy</td>\n",
       "      <td>t5_x5zpe</td>\n",
       "      <td>MadridBarcelonaLive</td>\n",
       "      <td>1551140514</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>t3_aunzqy</td>\n",
       "      <td>t5_x5zpe</td>\n",
       "      <td>MadridBarcelonaLive</td>\n",
       "      <td>1551140578</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>t3_aunzqy</td>\n",
       "      <td>t5_x5zpe</td>\n",
       "      <td>MadridBarcelonaLive</td>\n",
       "      <td>1551140622</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        parent_id subreddit_id            subreddit  created_utc     author  \\\n",
       "499995  t3_aunzqy     t5_x5zpe  MadridBarcelonaLive   1551140268  [deleted]   \n",
       "499996  t3_aunzqy     t5_x5zpe  MadridBarcelonaLive   1551140453  [deleted]   \n",
       "499997  t3_aunzqy     t5_x5zpe  MadridBarcelonaLive   1551140514  [deleted]   \n",
       "499998  t3_aunzqy     t5_x5zpe  MadridBarcelonaLive   1551140578  [deleted]   \n",
       "499999  t3_aunzqy     t5_x5zpe  MadridBarcelonaLive   1551140622  [deleted]   \n",
       "\n",
       "        score       body  \n",
       "499995      1  [removed]  \n",
       "499996      1  [removed]  \n",
       "499997      1  [removed]  \n",
       "499998      1  [removed]  \n",
       "499999      1  [removed]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130283</th>\n",
       "      <td>t3_amjoqf</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1549152624</td>\n",
       "      <td>DifficultDisplay1</td>\n",
       "      <td>13</td>\n",
       "      <td>Question, can someone tell me how many years e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129454</th>\n",
       "      <td>t3_anxlpr</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1549498595</td>\n",
       "      <td>Karakurizer</td>\n",
       "      <td>5</td>\n",
       "      <td>Nah the Wizards like their current purgatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126685</th>\n",
       "      <td>t3_as36gd</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1550535626</td>\n",
       "      <td>DrumzRUs</td>\n",
       "      <td>1</td>\n",
       "      <td>Curry's way more iconic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126599</th>\n",
       "      <td>t3_ankgtn</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1549411281</td>\n",
       "      <td>Karakurizer</td>\n",
       "      <td>1</td>\n",
       "      <td>They had to deal Cov for him. He will stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129227</th>\n",
       "      <td>t1_egp0dmi</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1550448112</td>\n",
       "      <td>rando1817</td>\n",
       "      <td>4</td>\n",
       "      <td>I may be petty and thatâ€™s not my only reason f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126301</th>\n",
       "      <td>t1_efu1hv5</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1549412327</td>\n",
       "      <td>retrohhh999</td>\n",
       "      <td>1</td>\n",
       "      <td>I want JaMychal Green too please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125161</th>\n",
       "      <td>t3_apabvg</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1549844494</td>\n",
       "      <td>carofdoom123</td>\n",
       "      <td>225</td>\n",
       "      <td>The Boston Acquaintances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128383</th>\n",
       "      <td>t1_efzg0wi</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1549585582</td>\n",
       "      <td>mkz419</td>\n",
       "      <td>2</td>\n",
       "      <td>only because it would be too obvious at that p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129959</th>\n",
       "      <td>t3_aoabqf</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1549584435</td>\n",
       "      <td>MyLadySansa</td>\n",
       "      <td>8</td>\n",
       "      <td>So, basically Bron gets first crack at both ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124808</th>\n",
       "      <td>t1_egoz9vr</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>nba</td>\n",
       "      <td>1550448045</td>\n",
       "      <td>ineedmorechainsaws</td>\n",
       "      <td>69</td>\n",
       "      <td>You're right that they're no different.  The i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         parent_id subreddit_id subreddit  created_utc              author  \\\n",
       "130283   t3_amjoqf     t5_2qo4s       nba   1549152624   DifficultDisplay1   \n",
       "129454   t3_anxlpr     t5_2qo4s       nba   1549498595         Karakurizer   \n",
       "126685   t3_as36gd     t5_2qo4s       nba   1550535626            DrumzRUs   \n",
       "126599   t3_ankgtn     t5_2qo4s       nba   1549411281         Karakurizer   \n",
       "129227  t1_egp0dmi     t5_2qo4s       nba   1550448112           rando1817   \n",
       "126301  t1_efu1hv5     t5_2qo4s       nba   1549412327         retrohhh999   \n",
       "125161   t3_apabvg     t5_2qo4s       nba   1549844494        carofdoom123   \n",
       "128383  t1_efzg0wi     t5_2qo4s       nba   1549585582              mkz419   \n",
       "129959   t3_aoabqf     t5_2qo4s       nba   1549584435         MyLadySansa   \n",
       "124808  t1_egoz9vr     t5_2qo4s       nba   1550448045  ineedmorechainsaws   \n",
       "\n",
       "        score                                               body  \n",
       "130283     13  Question, can someone tell me how many years e...  \n",
       "129454      5      Nah the Wizards like their current purgatory   \n",
       "126685      1                           Curry's way more iconic   \n",
       "126599      1        They had to deal Cov for him. He will stay   \n",
       "129227      4  I may be petty and thatâ€™s not my only reason f...  \n",
       "126301      1                   I want JaMychal Green too please  \n",
       "125161    225                           The Boston Acquaintances  \n",
       "128383      2  only because it would be too obvious at that p...  \n",
       "129959      8  So, basically Bron gets first crack at both ro...  \n",
       "124808     69  You're right that they're no different.  The i...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.subreddit==\"nba\"].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6307"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.subreddit==\"nba\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit\n",
      "AskReddit    23861\n",
      "politics      7724\n",
      "nba           5794\n",
      "nfl           5157\n",
      "funny         4093\n",
      "Name: body, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (data.groupby('subreddit')['body'].nunique().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AskReddit</th>\n",
       "      <td>25706</td>\n",
       "      <td>25706</td>\n",
       "      <td>25706</td>\n",
       "      <td>25706</td>\n",
       "      <td>25706</td>\n",
       "      <td>25706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>8845</td>\n",
       "      <td>8845</td>\n",
       "      <td>8845</td>\n",
       "      <td>8845</td>\n",
       "      <td>8845</td>\n",
       "      <td>8845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nba</th>\n",
       "      <td>6307</td>\n",
       "      <td>6307</td>\n",
       "      <td>6307</td>\n",
       "      <td>6307</td>\n",
       "      <td>6307</td>\n",
       "      <td>6307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nfl</th>\n",
       "      <td>5595</td>\n",
       "      <td>5595</td>\n",
       "      <td>5595</td>\n",
       "      <td>5595</td>\n",
       "      <td>5595</td>\n",
       "      <td>5595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>4544</td>\n",
       "      <td>4544</td>\n",
       "      <td>4544</td>\n",
       "      <td>4544</td>\n",
       "      <td>4544</td>\n",
       "      <td>4544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FortNiteBR</th>\n",
       "      <td>4022</td>\n",
       "      <td>4022</td>\n",
       "      <td>4022</td>\n",
       "      <td>4022</td>\n",
       "      <td>4022</td>\n",
       "      <td>4022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The_Donald</th>\n",
       "      <td>3889</td>\n",
       "      <td>3889</td>\n",
       "      <td>3889</td>\n",
       "      <td>3889</td>\n",
       "      <td>3889</td>\n",
       "      <td>3887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dankmemes</th>\n",
       "      <td>3809</td>\n",
       "      <td>3809</td>\n",
       "      <td>3809</td>\n",
       "      <td>3809</td>\n",
       "      <td>3809</td>\n",
       "      <td>3809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apexlegends</th>\n",
       "      <td>3750</td>\n",
       "      <td>3750</td>\n",
       "      <td>3750</td>\n",
       "      <td>3750</td>\n",
       "      <td>3750</td>\n",
       "      <td>3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memes</th>\n",
       "      <td>3707</td>\n",
       "      <td>3707</td>\n",
       "      <td>3707</td>\n",
       "      <td>3707</td>\n",
       "      <td>3707</td>\n",
       "      <td>3707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaming</th>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teenagers</th>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AskOuija</th>\n",
       "      <td>3155</td>\n",
       "      <td>3155</td>\n",
       "      <td>3155</td>\n",
       "      <td>3155</td>\n",
       "      <td>3155</td>\n",
       "      <td>3155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hockey</th>\n",
       "      <td>3061</td>\n",
       "      <td>3061</td>\n",
       "      <td>3061</td>\n",
       "      <td>3061</td>\n",
       "      <td>3061</td>\n",
       "      <td>3061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pics</th>\n",
       "      <td>2965</td>\n",
       "      <td>2965</td>\n",
       "      <td>2965</td>\n",
       "      <td>2965</td>\n",
       "      <td>2965</td>\n",
       "      <td>2965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unpopularopinion</th>\n",
       "      <td>2917</td>\n",
       "      <td>2917</td>\n",
       "      <td>2917</td>\n",
       "      <td>2917</td>\n",
       "      <td>2917</td>\n",
       "      <td>2917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NintendoSwitch</th>\n",
       "      <td>2890</td>\n",
       "      <td>2890</td>\n",
       "      <td>2890</td>\n",
       "      <td>2890</td>\n",
       "      <td>2890</td>\n",
       "      <td>2890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worldnews</th>\n",
       "      <td>2859</td>\n",
       "      <td>2859</td>\n",
       "      <td>2859</td>\n",
       "      <td>2859</td>\n",
       "      <td>2859</td>\n",
       "      <td>2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Showerthoughts</th>\n",
       "      <td>2706</td>\n",
       "      <td>2706</td>\n",
       "      <td>2706</td>\n",
       "      <td>2706</td>\n",
       "      <td>2706</td>\n",
       "      <td>2706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>2655</td>\n",
       "      <td>2655</td>\n",
       "      <td>2655</td>\n",
       "      <td>2655</td>\n",
       "      <td>2655</td>\n",
       "      <td>2655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aww</th>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "      <td>2484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>todayilearned</th>\n",
       "      <td>2295</td>\n",
       "      <td>2295</td>\n",
       "      <td>2295</td>\n",
       "      <td>2295</td>\n",
       "      <td>2295</td>\n",
       "      <td>2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SquaredCircle</th>\n",
       "      <td>2147</td>\n",
       "      <td>2147</td>\n",
       "      <td>2147</td>\n",
       "      <td>2147</td>\n",
       "      <td>2147</td>\n",
       "      <td>2147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnthemTheGame</th>\n",
       "      <td>2129</td>\n",
       "      <td>2129</td>\n",
       "      <td>2129</td>\n",
       "      <td>2129</td>\n",
       "      <td>2129</td>\n",
       "      <td>2129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AmItheAsshole</th>\n",
       "      <td>2093</td>\n",
       "      <td>2093</td>\n",
       "      <td>2093</td>\n",
       "      <td>2093</td>\n",
       "      <td>2093</td>\n",
       "      <td>2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gonewild</th>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PewdiepieSubmissions</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leagueoflegends</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Market76</th>\n",
       "      <td>1723</td>\n",
       "      <td>1723</td>\n",
       "      <td>1723</td>\n",
       "      <td>1723</td>\n",
       "      <td>1723</td>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movies</th>\n",
       "      <td>1704</td>\n",
       "      <td>1704</td>\n",
       "      <td>1704</td>\n",
       "      <td>1704</td>\n",
       "      <td>1704</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TESVI</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF2Servers</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF2fashionadvice</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF591</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFOLs</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TForceNetwork</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGReBirthCommunity</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THEKEYOFFICIAL</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TI_Calculators</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symphogear</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SwordAndScale</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SweetErinStream</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SurpriseSal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SuperMegaBaseball</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SuperShibe</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Super_Robot_Wars</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupermodelDogs</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportFromPewDiePie</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SupportiveWaifus</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SurfaceLinux</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SurvivalGaming</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SweatyGirls_NSFW</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SurvivalMode</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SurvivingMyInfidelity</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SushiAbomination</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suspiria</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suunto</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SuzukazeAoba</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SwearFreeDiscussion</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zztails</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20103 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       parent_id  subreddit_id  created_utc  author  score  \\\n",
       "subreddit                                                                    \n",
       "AskReddit                  25706         25706        25706   25706  25706   \n",
       "politics                    8845          8845         8845    8845   8845   \n",
       "nba                         6307          6307         6307    6307   6307   \n",
       "nfl                         5595          5595         5595    5595   5595   \n",
       "funny                       4544          4544         4544    4544   4544   \n",
       "FortNiteBR                  4022          4022         4022    4022   4022   \n",
       "The_Donald                  3889          3889         3889    3889   3889   \n",
       "dankmemes                   3809          3809         3809    3809   3809   \n",
       "apexlegends                 3750          3750         3750    3750   3750   \n",
       "memes                       3707          3707         3707    3707   3707   \n",
       "gaming                      3648          3648         3648    3648   3648   \n",
       "teenagers                   3476          3476         3476    3476   3476   \n",
       "AskOuija                    3155          3155         3155    3155   3155   \n",
       "hockey                      3061          3061         3061    3061   3061   \n",
       "pics                        2965          2965         2965    2965   2965   \n",
       "unpopularopinion            2917          2917         2917    2917   2917   \n",
       "NintendoSwitch              2890          2890         2890    2890   2890   \n",
       "worldnews                   2859          2859         2859    2859   2859   \n",
       "Showerthoughts              2706          2706         2706    2706   2706   \n",
       "news                        2655          2655         2655    2655   2655   \n",
       "aww                         2484          2484         2484    2484   2484   \n",
       "todayilearned               2295          2295         2295    2295   2295   \n",
       "SquaredCircle               2147          2147         2147    2147   2147   \n",
       "AnthemTheGame               2129          2129         2129    2129   2129   \n",
       "AmItheAsshole               2093          2093         2093    2093   2093   \n",
       "gonewild                    2072          2072         2072    2072   2072   \n",
       "PewdiepieSubmissions        2019          2019         2019    2019   2019   \n",
       "leagueoflegends             2005          2005         2005    2005   2005   \n",
       "Market76                    1723          1723         1723    1723   1723   \n",
       "movies                      1704          1704         1704    1704   1704   \n",
       "...                          ...           ...          ...     ...    ...   \n",
       "TESVI                          1             1            1       1      1   \n",
       "TF2Servers                     1             1            1       1      1   \n",
       "TF2fashionadvice               1             1            1       1      1   \n",
       "TF591                          1             1            1       1      1   \n",
       "TFOLs                          1             1            1       1      1   \n",
       "TForceNetwork                  1             1            1       1      1   \n",
       "TGReBirthCommunity             1             1            1       1      1   \n",
       "THEKEYOFFICIAL                 1             1            1       1      1   \n",
       "TI_Calculators                 1             1            1       1      1   \n",
       "Symphogear                     1             1            1       1      1   \n",
       "SwordAndScale                  1             1            1       1      1   \n",
       "SweetErinStream                1             1            1       1      1   \n",
       "SurpriseSal                    1             1            1       1      1   \n",
       "SuperMegaBaseball              1             1            1       1      1   \n",
       "SuperShibe                     1             1            1       1      1   \n",
       "Super_Robot_Wars               1             1            1       1      1   \n",
       "SupermodelDogs                 1             1            1       1      1   \n",
       "SupportFromPewDiePie           1             1            1       1      1   \n",
       "SupportiveWaifus               1             1            1       1      1   \n",
       "SurfaceLinux                   1             1            1       1      1   \n",
       "SurvivalGaming                 1             1            1       1      1   \n",
       "SweatyGirls_NSFW               1             1            1       1      1   \n",
       "SurvivalMode                   1             1            1       1      1   \n",
       "SurvivingMyInfidelity          1             1            1       1      1   \n",
       "SushiAbomination               1             1            1       1      1   \n",
       "Suspiria                       1             1            1       1      1   \n",
       "Suunto                         1             1            1       1      1   \n",
       "SuzukazeAoba                   1             1            1       1      1   \n",
       "SwearFreeDiscussion            1             1            1       1      1   \n",
       "zztails                        1             1            1       1      1   \n",
       "\n",
       "                        body  \n",
       "subreddit                     \n",
       "AskReddit              25706  \n",
       "politics                8845  \n",
       "nba                     6307  \n",
       "nfl                     5595  \n",
       "funny                   4544  \n",
       "FortNiteBR              4022  \n",
       "The_Donald              3887  \n",
       "dankmemes               3809  \n",
       "apexlegends             3750  \n",
       "memes                   3707  \n",
       "gaming                  3648  \n",
       "teenagers               3476  \n",
       "AskOuija                3155  \n",
       "hockey                  3061  \n",
       "pics                    2965  \n",
       "unpopularopinion        2917  \n",
       "NintendoSwitch          2890  \n",
       "worldnews               2859  \n",
       "Showerthoughts          2706  \n",
       "news                    2655  \n",
       "aww                     2484  \n",
       "todayilearned           2295  \n",
       "SquaredCircle           2147  \n",
       "AnthemTheGame           2129  \n",
       "AmItheAsshole           2093  \n",
       "gonewild                2072  \n",
       "PewdiepieSubmissions    2019  \n",
       "leagueoflegends         2005  \n",
       "Market76                1723  \n",
       "movies                  1704  \n",
       "...                      ...  \n",
       "TESVI                      1  \n",
       "TF2Servers                 1  \n",
       "TF2fashionadvice           1  \n",
       "TF591                      1  \n",
       "TFOLs                      1  \n",
       "TForceNetwork              1  \n",
       "TGReBirthCommunity         1  \n",
       "THEKEYOFFICIAL             1  \n",
       "TI_Calculators             1  \n",
       "Symphogear                 1  \n",
       "SwordAndScale              1  \n",
       "SweetErinStream            1  \n",
       "SurpriseSal                1  \n",
       "SuperMegaBaseball          1  \n",
       "SuperShibe                 1  \n",
       "Super_Robot_Wars           1  \n",
       "SupermodelDogs             1  \n",
       "SupportFromPewDiePie       1  \n",
       "SupportiveWaifus           1  \n",
       "SurfaceLinux               1  \n",
       "SurvivalGaming             1  \n",
       "SweatyGirls_NSFW           1  \n",
       "SurvivalMode               1  \n",
       "SurvivingMyInfidelity      1  \n",
       "SushiAbomination           1  \n",
       "Suspiria                   1  \n",
       "Suunto                     1  \n",
       "SuzukazeAoba               1  \n",
       "SwearFreeDiscussion        1  \n",
       "zztails                    1  \n",
       "\n",
       "[20103 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('subreddit').agg('count').sort_values('body', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change created_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1549239892"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.created_utc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 2, 4, 0, 24, 52)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_date = datetime.utcfromtimestamp(data.created_utc[1])\n",
    "parsed_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = parsed_date.year\n",
    "month = parsed_date.month\n",
    "day = parsed_date.day\n",
    "hour = parsed_date.hour\n",
    "minute = parsed_date.minute\n",
    "second = parsed_date.second\n",
    "second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2019-02-03 00:20:05\n",
       "1        2019-02-04 00:24:52\n",
       "2        2019-02-14 00:04:21\n",
       "3        2019-02-03 00:14:05\n",
       "4        2019-02-12 00:13:01\n",
       "5        2019-02-03 00:14:54\n",
       "6        2019-02-14 00:17:10\n",
       "7        2019-02-15 00:02:30\n",
       "8        2019-02-22 00:10:10\n",
       "9        2019-02-19 00:19:09\n",
       "10       2019-02-28 00:01:08\n",
       "11       2019-02-09 00:07:08\n",
       "12       2019-02-19 00:24:55\n",
       "13       2019-02-03 00:26:36\n",
       "14       2019-02-01 00:01:12\n",
       "15       2019-02-02 00:08:24\n",
       "16       2019-02-16 00:11:26\n",
       "17       2019-02-18 00:03:03\n",
       "18       2019-02-21 00:08:12\n",
       "19       2019-02-02 00:09:14\n",
       "20       2019-02-02 00:13:50\n",
       "21       2019-02-21 00:18:31\n",
       "22       2019-02-12 00:18:14\n",
       "23       2019-02-07 00:29:27\n",
       "24       2019-02-03 00:05:14\n",
       "25       2019-02-23 00:28:53\n",
       "26       2019-02-07 00:26:42\n",
       "27       2019-02-02 00:01:43\n",
       "28       2019-02-10 00:21:25\n",
       "29       2019-02-04 00:17:32\n",
       "                 ...        \n",
       "499970   2019-02-26 00:14:24\n",
       "499971   2019-02-26 00:14:43\n",
       "499972   2019-02-26 00:14:45\n",
       "499973   2019-02-26 00:15:08\n",
       "499974   2019-02-26 00:16:31\n",
       "499975   2019-02-26 00:17:22\n",
       "499976   2019-02-26 00:17:26\n",
       "499977   2019-02-26 00:17:47\n",
       "499978   2019-02-26 00:18:44\n",
       "499979   2019-02-26 00:21:25\n",
       "499980   2019-02-26 00:23:08\n",
       "499981   2019-02-26 00:25:25\n",
       "499982   2019-02-26 00:25:28\n",
       "499983   2019-02-26 00:28:32\n",
       "499984   2019-02-26 00:07:09\n",
       "499985   2019-02-26 00:07:26\n",
       "499986   2019-02-26 00:07:45\n",
       "499987   2019-02-26 00:08:19\n",
       "499988   2019-02-26 00:08:39\n",
       "499989   2019-02-26 00:09:03\n",
       "499990   2019-02-26 00:10:36\n",
       "499991   2019-02-26 00:10:52\n",
       "499992   2019-02-26 00:16:55\n",
       "499993   2019-02-26 00:16:56\n",
       "499994   2019-02-26 00:17:24\n",
       "499995   2019-02-26 00:17:48\n",
       "499996   2019-02-26 00:20:53\n",
       "499997   2019-02-26 00:21:54\n",
       "499998   2019-02-26 00:22:58\n",
       "499999   2019-02-26 00:23:42\n",
       "Name: created_utc, Length: 500000, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.created_utc.apply(lambda x:datetime.utcfromtimestamp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words with NLTK & counting each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'READ THIS MESSAGE! We require a minimum account-age and karma.  ***These minimums are not disclosed***.  No exceptions will be made.  Messaging mods for the minimums will result in a 30-day ban.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/nsfw) if you have any questions or concerns.*'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.body[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliekim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'read this message we require a minimum account age and karma these minimums are not disclosed no exceptions will be made messaging mods for the minimums will result in a 30 day ban i am a bot and this action was performed automatically please contact the moderators of this subreddit message compose to r nsfw if you have any questions or concerns '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.body[13] = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', k) for k in data.body[13].lower().split(\"\\n\")]\n",
    "data.body[13] = re.sub(r'\\W+', ' ', data.body[13].lower())\n",
    "data.body[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'this', 'message', 'we', 'require', 'a', 'minimum', 'account', 'age', 'and', 'karma', 'these', 'minimums', 'are', 'not', 'disclosed', 'no', 'exceptions', 'will', 'be', 'made', 'messaging', 'mods', 'for', 'the', 'minimums', 'will', 'result', 'in', 'a', '30', 'day', 'ban', 'i', 'am', 'a', 'bot', 'and', 'this', 'action', 'was', 'performed', 'automatically', 'please', 'contact', 'the', 'moderators', 'of', 'this', 'subreddit', 'message', 'compose', 'to', 'r', 'nsfw', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns']\n",
      "['read', 'message', 'require', 'minimum', 'account', 'age', 'karma', 'minimums', 'disclosed', 'exceptions', 'made', 'messaging', 'mods', 'minimums', 'result', '30', 'day', 'ban', 'bot', 'action', 'performed', 'automatically', 'please', 'contact', 'moderators', 'subreddit', 'message', 'compose', 'r', 'nsfw', 'questions', 'concerns']\n",
      "{'read': 1, 'message': 2, 'require': 1, 'minimum': 1, 'account': 1, 'age': 1, 'karma': 1, 'minimums': 2, 'disclosed': 1, 'exceptions': 1, 'made': 1, 'messaging': 1, 'mods': 1, 'result': 1, '30': 1, 'day': 1, 'ban': 1, 'bot': 1, 'action': 1, 'performed': 1, 'automatically': 1, 'please': 1, 'contact': 1, 'moderators': 1, 'subreddit': 1, 'compose': 1, 'r': 1, 'nsfw': 1, 'questions': 1, 'concerns': 1}\n"
     ]
    }
   ],
   "source": [
    "example_sent = data.body[13]\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(data.body[13]) \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "filtered_sentence = []  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w)   \n",
    "print(word_tokens) \n",
    "print(filtered_sentence) \n",
    "\n",
    "words = filtered_sentence\n",
    "counts = dict()\n",
    "for word in words:\n",
    "    if word in counts:\n",
    "        counts[word] += 1\n",
    "\n",
    "    else:\n",
    "        counts[word] = 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean comments function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    sentence = re.sub(r'\\W+', ' ', sentence.lower())\n",
    "    return sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"I'm going to swim, and then eat. How about you?\"\n",
    "clean(test1)\n",
    "# num_word = group.body.apply(lambda x: word_count(x))\n",
    "clean_df = group.body.apply(lambda x: clean(x))\n",
    "clean_df= pd.DataFrame(clean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u w u so far it goes to mk4 but i dont have t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i think that s a good idea her constant bad us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey there looks like you re not using our stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ok no problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>well deserved if my girls look that good at 40...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0   u w u so far it goes to mk4 but i dont have t...\n",
       "1  i think that s a good idea her constant bad us...\n",
       "2  hey there looks like you re not using our stan...\n",
       "3                                      ok no problem\n",
       "4  well deserved if my girls look that good at 40..."
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_func(sentence):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[u, w, u, far, goes, mk4, dont, drawings, mk3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[think, good, idea, constant, bad, use, photos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hey, looks, like, using, standard, flair, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ok, problem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[well, deserved, girls, look, good, 40, smilin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  [u, w, u, far, goes, mk4, dont, drawings, mk3, 4]\n",
       "1  [think, good, idea, constant, bad, use, photos...\n",
       "2  [hey, looks, like, using, standard, flair, for...\n",
       "3                                      [ok, problem]\n",
       "4  [well, deserved, girls, look, good, 40, smilin..."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test1 = \"I'm going to swim, and then eat. How about you?\"\n",
    "# token_func(clean(test1))\n",
    "token_df = group.body.apply(lambda x: token_func(clean(x)))\n",
    "token_df= pd.DataFrame(token_df)\n",
    "token_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_count(sentence):\n",
    "    sentence = re.sub(r'\\W+', ' ', sentence.lower())\n",
    "    counts = dict()\n",
    "    \n",
    "    ### Removing stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    ###Word count\n",
    "    words = filtered_sentence\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by Subreddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pgroup = data.groupby(['subreddit'], as_index = False)\n",
    "# new_data = pgroup.apply(lambda x: x['body'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[['subreddit', 'body']].groupby('subreddit')\n",
    "# data['combined'] = data['body'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = (data.groupby('subreddit')['body'].apply(lambda x: ','.join(set(x.dropna()))).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0w0</td>\n",
       "      <td>so far it goes to MK4 but i dont have the draw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000ccplus</td>\n",
       "      <td>What did it say lol,Her constant (bad) use of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000daysofpractice</td>\n",
       "      <td>Music 27\\n\\ntwo hours, played the first page o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100ThievesApparel</td>\n",
       "      <td>ok no problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100sexiest</td>\n",
       "      <td>Well deserved....if my girls look that good at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100thieves</td>\n",
       "      <td>Like I'm all for picking unconventional lineup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100yearsago</td>\n",
       "      <td>Yup, it made think of my little brother wearin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>101Wicca</td>\n",
       "      <td>it's fine. We accept everyone from everywhere ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                               body\n",
       "0                 0w0  so far it goes to MK4 but i dont have the draw...\n",
       "1          1000ccplus  What did it say lol,Her constant (bad) use of ...\n",
       "2  1000daysofpractice  Music 27\\n\\ntwo hours, played the first page o...\n",
       "3   100ThievesApparel                                      ok no problem\n",
       "4          100sexiest  Well deserved....if my girls look that good at...\n",
       "5          100thieves  Like I'm all for picking unconventional lineup...\n",
       "6         100yearsago  Yup, it made think of my little brother wearin...\n",
       "7            101Wicca  it's fine. We accept everyone from everywhere ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20103, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yup, it made think of my little brother wearing my grandfatherâ€™s WWl helmet when he was a small boy.,Strike on the Tube. Nothing ever changes. ,what a different world had the US gone in on the German side....and there was talk'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.body[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# new_data[6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word count for each subreddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_word = group.body.apply(lambda x: word_count(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'far': 1, 'goes': 1, 'mk4': 1, 'dont': 1, 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'say': 1, 'lol': 1, 'constant': 1, 'bad': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'music': 1, '27': 1, 'two': 1, 'hours': 1, 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'ok': 1, 'problem': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'well': 1, 'deserved': 1, 'girls': 1, 'look':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  {'far': 1, 'goes': 1, 'mk4': 1, 'dont': 1, 'dr...\n",
       "1  {'say': 1, 'lol': 1, 'constant': 1, 'bad': 1, ...\n",
       "2  {'music': 1, '27': 1, 'two': 1, 'hours': 1, 'p...\n",
       "3                            {'ok': 1, 'problem': 1}\n",
       "4  {'well': 1, 'deserved': 1, 'girls': 1, 'look':..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_word= pd.DataFrame(num_word)\n",
    "num_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>CultOfGuac</td>\n",
       "      <td>[(ahaha, 1), (amp, 1), (x200b, 1), (yaaas, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7170</th>\n",
       "      <td>OldPhotosInRealLife</td>\n",
       "      <td>[(side, 2), (r, 1), (1healthbar, 1), (like, 1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5489</th>\n",
       "      <td>KingBuffalo</td>\n",
       "      <td>[(rock, 2), (favorite, 2), (well, 2), (music, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>JessicaNigri</td>\n",
       "      <td>[(cosplay, 2), (think, 2), (ever, 2), (general...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19274</th>\n",
       "      <td>u_mobilemechanicrgv</td>\n",
       "      <td>[(tx, 64), (towing, 42), (edinburg, 36), (mcal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>alpha</td>\n",
       "      <td>[(true, 1), (alpha, 1), (males, 1), (upvote, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395</th>\n",
       "      <td>rs2vietnam</td>\n",
       "      <td>[(game, 13), (gt, 10), (gameplay, 6), (like, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16770</th>\n",
       "      <td>pkmntcgreferences</td>\n",
       "      <td>[(cards, 2), (great, 2), (would, 2), (recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8242</th>\n",
       "      <td>RedditSilverRobot</td>\n",
       "      <td>[(redditbronze, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19088</th>\n",
       "      <td>u_TTVunknown_phantomYT</td>\n",
       "      <td>[(would, 1), (like, 1), (join, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>freediving</td>\n",
       "      <td>[(http, 3), (called, 3), (www, 2), (polosub, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10319</th>\n",
       "      <td>UnbiasedCanada</td>\n",
       "      <td>[(government, 8), (people, 7), (say, 5), (meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8530</th>\n",
       "      <td>SWGOHRecruiting</td>\n",
       "      <td>[(hale, 3), (big, 3), (brother, 3), (guild, 3)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7853</th>\n",
       "      <td>Princess_Olivia</td>\n",
       "      <td>[(sold, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>POIS</td>\n",
       "      <td>[(subclinical, 2), (yes, 1), (hypothyroid, 1),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Subreddit  \\\n",
       "2499               CultOfGuac   \n",
       "7170      OldPhotosInRealLife   \n",
       "5489              KingBuffalo   \n",
       "5260             JessicaNigri   \n",
       "19274     u_mobilemechanicrgv   \n",
       "11264                   alpha   \n",
       "17395              rs2vietnam   \n",
       "16770       pkmntcgreferences   \n",
       "8242        RedditSilverRobot   \n",
       "19088  u_TTVunknown_phantomYT   \n",
       "13848              freediving   \n",
       "10319          UnbiasedCanada   \n",
       "8530          SWGOHRecruiting   \n",
       "7853          Princess_Olivia   \n",
       "7358                     POIS   \n",
       "\n",
       "                                                  Values  \n",
       "2499      [(ahaha, 1), (amp, 1), (x200b, 1), (yaaas, 1)]  \n",
       "7170   [(side, 2), (r, 1), (1healthbar, 1), (like, 1)...  \n",
       "5489   [(rock, 2), (favorite, 2), (well, 2), (music, ...  \n",
       "5260   [(cosplay, 2), (think, 2), (ever, 2), (general...  \n",
       "19274  [(tx, 64), (towing, 42), (edinburg, 36), (mcal...  \n",
       "11264  [(true, 1), (alpha, 1), (males, 1), (upvote, 1...  \n",
       "17395  [(game, 13), (gt, 10), (gameplay, 6), (like, 5...  \n",
       "16770  [(cards, 2), (great, 2), (would, 2), (recommen...  \n",
       "8242                                 [(redditbronze, 1)]  \n",
       "19088                 [(would, 1), (like, 1), (join, 1)]  \n",
       "13848  [(http, 3), (called, 3), (www, 2), (polosub, 2...  \n",
       "10319  [(government, 8), (people, 7), (say, 5), (meas...  \n",
       "8530   [(hale, 3), (big, 3), (brother, 3), (guild, 3)...  \n",
       "7853                                         [(sold, 1)]  \n",
       "7358   [(subclinical, 2), (yes, 1), (hypothyroid, 1),...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([group['subreddit'], num_word['body']], axis=1, keys=['Subreddit', 'Values'])\n",
    "df['Values'] = df['Values'].map(lambda t: sorted(t.items(), key = lambda x: x[1],reverse=True))\n",
    "# df.sort_values(['Values'], ascending=[False])\n",
    "df.sample(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('like', 606),\n",
       "  ('would', 513),\n",
       "  ('team', 510),\n",
       "  ('get', 421),\n",
       "  ('lebron', 379),\n",
       "  ('think', 373),\n",
       "  ('good', 327),\n",
       "  ('year', 325),\n",
       "  ('one', 323),\n",
       "  ('even', 317),\n",
       "  ('game', 311),\n",
       "  ('players', 308),\n",
       "  ('better', 302),\n",
       "  ('player', 300),\n",
       "  ('nba', 274),\n",
       "  ('lakers', 273),\n",
       "  ('gt', 270),\n",
       "  ('still', 250),\n",
       "  ('got', 249),\n",
       "  ('lol', 249),\n",
       "  ('really', 246),\n",
       "  ('people', 245),\n",
       "  ('time', 234),\n",
       "  ('trade', 227),\n",
       "  ('know', 223),\n",
       "  ('best', 219),\n",
       "  ('ad', 217),\n",
       "  ('teams', 216),\n",
       "  ('play', 215),\n",
       "  ('season', 212),\n",
       "  ('could', 206),\n",
       "  ('see', 205),\n",
       "  ('much', 203),\n",
       "  ('going', 197),\n",
       "  ('3', 194),\n",
       "  ('right', 193),\n",
       "  ('make', 191),\n",
       "  ('last', 191),\n",
       "  ('want', 188),\n",
       "  ('go', 180),\n",
       "  ('also', 180),\n",
       "  ('way', 177),\n",
       "  ('guys', 169),\n",
       "  ('years', 167),\n",
       "  ('star', 167),\n",
       "  ('well', 167),\n",
       "  ('said', 160),\n",
       "  ('5', 157),\n",
       "  ('league', 156),\n",
       "  ('first', 154),\n",
       "  ('bad', 151),\n",
       "  ('say', 150),\n",
       "  ('shit', 147),\n",
       "  ('man', 147),\n",
       "  ('point', 145),\n",
       "  ('yeah', 139),\n",
       "  ('guy', 139),\n",
       "  ('1', 139),\n",
       "  ('great', 138),\n",
       "  ('games', 137),\n",
       "  ('never', 136),\n",
       "  ('2', 136),\n",
       "  ('back', 135),\n",
       "  ('probably', 135),\n",
       "  ('top', 135),\n",
       "  ('na', 134),\n",
       "  ('fuck', 134),\n",
       "  ('take', 131),\n",
       "  ('every', 131),\n",
       "  ('lot', 127),\n",
       "  ('kyrie', 125),\n",
       "  ('playing', 125),\n",
       "  ('saying', 124),\n",
       "  ('basketball', 123),\n",
       "  ('sure', 120),\n",
       "  ('gon', 119),\n",
       "  ('two', 119),\n",
       "  ('big', 118),\n",
       "  ('fans', 117),\n",
       "  ('lmao', 117),\n",
       "  ('giannis', 117),\n",
       "  ('actually', 117),\n",
       "  ('win', 114),\n",
       "  ('contract', 112),\n",
       "  ('getting', 111),\n",
       "  ('mean', 110),\n",
       "  ('made', 108),\n",
       "  ('though', 107),\n",
       "  ('give', 105),\n",
       "  ('something', 105),\n",
       "  ('watch', 104),\n",
       "  ('kd', 102),\n",
       "  ('pick', 102),\n",
       "  ('let', 101),\n",
       "  ('fan', 101),\n",
       "  ('4', 101),\n",
       "  ('played', 100),\n",
       "  ('playoffs', 99),\n",
       "  ('pretty', 98),\n",
       "  ('harden', 98),\n",
       "  ('us', 98),\n",
       "  ('need', 97),\n",
       "  ('knicks', 96),\n",
       "  ('next', 96),\n",
       "  ('celtics', 95),\n",
       "  ('ever', 95),\n",
       "  ('look', 94),\n",
       "  ('thing', 92),\n",
       "  ('amp', 90),\n",
       "  ('fucking', 90),\n",
       "  ('love', 89),\n",
       "  ('free', 89),\n",
       "  ('always', 89),\n",
       "  ('thought', 88),\n",
       "  ('paul', 87),\n",
       "  ('someone', 86),\n",
       "  ('without', 86),\n",
       "  ('dude', 85),\n",
       "  ('ball', 85),\n",
       "  ('everyone', 85),\n",
       "  ('traded', 85),\n",
       "  ('around', 83),\n",
       "  ('money', 83),\n",
       "  ('warriors', 82),\n",
       "  ('new', 82),\n",
       "  ('kawhi', 82),\n",
       "  ('long', 81),\n",
       "  ('la', 80),\n",
       "  ('come', 80),\n",
       "  ('put', 79),\n",
       "  ('makes', 78),\n",
       "  ('pelicans', 78),\n",
       "  ('young', 78),\n",
       "  ('deal', 77),\n",
       "  ('post', 77),\n",
       "  ('offer', 77),\n",
       "  ('since', 76),\n",
       "  ('might', 75),\n",
       "  ('went', 75),\n",
       "  ('tatum', 75),\n",
       "  ('picks', 75),\n",
       "  ('already', 74),\n",
       "  ('many', 73),\n",
       "  ('klay', 72),\n",
       "  ('second', 72),\n",
       "  ('anything', 72),\n",
       "  ('another', 72),\n",
       "  ('reason', 71),\n",
       "  ('summer', 69),\n",
       "  ('gets', 69),\n",
       "  ('https', 69),\n",
       "  ('literally', 69),\n",
       "  ('maybe', 67),\n",
       "  ('draft', 67),\n",
       "  ('done', 67),\n",
       "  ('keep', 67),\n",
       "  ('trying', 66),\n",
       "  ('boston', 66),\n",
       "  ('enough', 65),\n",
       "  ('minutes', 65),\n",
       "  ('6', 64),\n",
       "  ('magic', 64),\n",
       "  ('yes', 64),\n",
       "  ('7', 63),\n",
       "  ('defense', 63),\n",
       "  ('oh', 63),\n",
       "  ('true', 63),\n",
       "  ('kobe', 63),\n",
       "  ('pg', 62),\n",
       "  ('com', 62),\n",
       "  ('feel', 61),\n",
       "  ('least', 61),\n",
       "  ('real', 61),\n",
       "  ('damn', 61),\n",
       "  ('10', 61),\n",
       "  ('old', 60),\n",
       "  ('talk', 59),\n",
       "  ('definitely', 59),\n",
       "  ('things', 59),\n",
       "  ('making', 59),\n",
       "  ('nothing', 59),\n",
       "  ('bucks', 58),\n",
       "  ('talking', 58),\n",
       "  ('wait', 58),\n",
       "  ('anyone', 57),\n",
       "  ('cap', 57),\n",
       "  ('value', 56),\n",
       "  ('guess', 56),\n",
       "  ('rich', 56),\n",
       "  ('either', 56),\n",
       "  ('r', 56),\n",
       "  ('bench', 56),\n",
       "  ('stats', 56),\n",
       "  ('playoff', 56),\n",
       "  ('chance', 55),\n",
       "  ('shot', 54),\n",
       "  ('sign', 54),\n",
       "  ('agree', 54),\n",
       "  ('zion', 54),\n",
       "  ('part', 53),\n",
       "  ('instead', 53),\n",
       "  ('max', 53),\n",
       "  ('fact', 53),\n",
       "  ('try', 53),\n",
       "  ('high', 53),\n",
       "  ('space', 53),\n",
       "  ('career', 52),\n",
       "  ('west', 52),\n",
       "  ('day', 51),\n",
       "  ('luka', 51),\n",
       "  ('move', 51),\n",
       "  ('wanted', 51),\n",
       "  ('seen', 51),\n",
       "  ('media', 50),\n",
       "  ('different', 50),\n",
       "  ('taking', 50),\n",
       "  ('less', 50),\n",
       "  ('westbrook', 50),\n",
       "  ('lonzo', 50),\n",
       "  ('away', 49),\n",
       "  ('hard', 49),\n",
       "  ('hate', 49),\n",
       "  ('level', 49),\n",
       "  ('kp', 49),\n",
       "  ('else', 49),\n",
       "  ('far', 49),\n",
       "  ('hope', 49),\n",
       "  ('looking', 49),\n",
       "  ('raptors', 49),\n",
       "  ('dont', 49),\n",
       "  ('wants', 48),\n",
       "  ('everything', 48),\n",
       "  ('work', 48),\n",
       "  ('injury', 48),\n",
       "  ('honestly', 48),\n",
       "  ('vs', 48),\n",
       "  ('points', 48),\n",
       "  ('tell', 47),\n",
       "  ('close', 47),\n",
       "  ('comment', 47),\n",
       "  ('winning', 47),\n",
       "  ('stars', 47),\n",
       "  ('court', 47),\n",
       "  ('finals', 47),\n",
       "  ('looks', 47),\n",
       "  ('basically', 46),\n",
       "  ('wrong', 46),\n",
       "  ('read', 46),\n",
       "  ('seems', 46),\n",
       "  ('almost', 46),\n",
       "  ('care', 45),\n",
       "  ('round', 45),\n",
       "  ('start', 45),\n",
       "  ('8', 45),\n",
       "  ('believe', 44),\n",
       "  ('happen', 44),\n",
       "  ('especially', 44),\n",
       "  ('sports', 43),\n",
       "  ('imagine', 43),\n",
       "  ('davis', 43),\n",
       "  ('call', 43),\n",
       "  ('used', 43),\n",
       "  ('east', 43),\n",
       "  ('exactly', 43),\n",
       "  ('ass', 43),\n",
       "  ('likely', 43),\n",
       "  ('little', 43),\n",
       "  ('kind', 42),\n",
       "  ('offense', 42),\n",
       "  ('1st', 42),\n",
       "  ('shooting', 42),\n",
       "  ('hell', 42),\n",
       "  ('injured', 42),\n",
       "  ('college', 42),\n",
       "  ('half', 41),\n",
       "  ('lowry', 41),\n",
       "  ('embiid', 41),\n",
       "  ('sub', 41),\n",
       "  ('front', 41),\n",
       "  ('gobert', 41),\n",
       "  ('curry', 41),\n",
       "  ('agent', 40),\n",
       "  ('rockets', 40),\n",
       "  ('steph', 40),\n",
       "  ('future', 40),\n",
       "  ('called', 40),\n",
       "  ('roster', 40),\n",
       "  ('series', 40),\n",
       "  ('list', 40),\n",
       "  ('sense', 39),\n",
       "  ('tampering', 39),\n",
       "  ('worse', 39),\n",
       "  ('nah', 39),\n",
       "  ('please', 39),\n",
       "  ('average', 39),\n",
       "  ('live', 39),\n",
       "  ('super', 39),\n",
       "  ('whole', 39),\n",
       "  ('history', 39),\n",
       "  ('nice', 39),\n",
       "  ('championship', 39),\n",
       "  ('yet', 39),\n",
       "  ('imo', 39),\n",
       "  ('coach', 39),\n",
       "  ('problem', 38),\n",
       "  ('beat', 38),\n",
       "  ('plus', 38),\n",
       "  ('thread', 38),\n",
       "  ('mvp', 38),\n",
       "  ('understand', 38),\n",
       "  ('kuzma', 38),\n",
       "  ('per', 38),\n",
       "  ('ainge', 38),\n",
       "  ('ingram', 38),\n",
       "  ('ok', 38),\n",
       "  ('obviously', 37),\n",
       "  ('run', 37),\n",
       "  ('name', 37),\n",
       "  ('times', 37),\n",
       "  ('cause', 37),\n",
       "  ('gm', 37),\n",
       "  ('clippers', 37),\n",
       "  ('worth', 37),\n",
       "  ('today', 37),\n",
       "  ('watching', 37),\n",
       "  ('defender', 37),\n",
       "  ('worst', 37),\n",
       "  ('stupid', 36),\n",
       "  ('okc', 36),\n",
       "  ('knows', 36),\n",
       "  ('stay', 36),\n",
       "  ('crazy', 36),\n",
       "  ('use', 36),\n",
       "  ('butler', 36),\n",
       "  ('remember', 36),\n",
       "  ('ago', 36),\n",
       "  ('means', 36),\n",
       "  ('life', 35),\n",
       "  ('lost', 35),\n",
       "  ('thats', 35),\n",
       "  ('show', 35),\n",
       "  ('30', 35),\n",
       "  ('lose', 35),\n",
       "  ('number', 35),\n",
       "  ('cavs', 35),\n",
       "  ('three', 35),\n",
       "  ('happened', 35),\n",
       "  ('15', 35),\n",
       "  ('100', 35),\n",
       "  ('needs', 35),\n",
       "  ('stop', 35),\n",
       "  ('franchise', 35),\n",
       "  ('pels', 35),\n",
       "  ('starting', 35),\n",
       "  ('mavs', 35),\n",
       "  ('2nd', 35),\n",
       "  ('entire', 35),\n",
       "  ('absolutely', 35),\n",
       "  ('goes', 35),\n",
       "  ('guard', 35),\n",
       "  ('coming', 34),\n",
       "  ('find', 34),\n",
       "  ('jordan', 34),\n",
       "  ('took', 34),\n",
       "  ('agents', 34),\n",
       "  ('god', 34),\n",
       "  ('20', 34),\n",
       "  ('bet', 34),\n",
       "  ('nobody', 33),\n",
       "  ('jazz', 33),\n",
       "  ('case', 33),\n",
       "  ('40', 33),\n",
       "  ('fun', 33),\n",
       "  ('simmons', 33),\n",
       "  ('million', 32),\n",
       "  ('night', 32),\n",
       "  ('option', 32),\n",
       "  ('may', 32),\n",
       "  ('etc', 32),\n",
       "  ('picked', 32),\n",
       "  ('days', 32),\n",
       "  ('hit', 31),\n",
       "  ('able', 31),\n",
       "  ('tho', 31),\n",
       "  ('rest', 31),\n",
       "  ('rather', 31),\n",
       "  ('sixers', 31),\n",
       "  ('possible', 31),\n",
       "  ('question', 31),\n",
       "  ('giving', 31),\n",
       "  ('talent', 31),\n",
       "  ('edit', 31),\n",
       "  ('past', 31),\n",
       "  ('seed', 31),\n",
       "  ('trash', 30),\n",
       "  ('small', 30),\n",
       "  ('face', 30),\n",
       "  ('potential', 30),\n",
       "  ('brown', 30),\n",
       "  ('came', 30),\n",
       "  ('pay', 30),\n",
       "  ('spot', 30),\n",
       "  ('miss', 30),\n",
       "  ('u', 30),\n",
       "  ('market', 30),\n",
       "  ('doubt', 30),\n",
       "  ('plays', 30),\n",
       "  ('world', 30),\n",
       "  ('goat', 30),\n",
       "  ('clearly', 29),\n",
       "  ('job', 29),\n",
       "  ('started', 29),\n",
       "  ('ask', 29),\n",
       "  ('early', 29),\n",
       "  ('numbers', 29),\n",
       "  ('james', 29),\n",
       "  ('amazing', 29),\n",
       "  ('salary', 29),\n",
       "  ('www', 29),\n",
       "  ('gasol', 29),\n",
       "  ('office', 29),\n",
       "  ('spurs', 29),\n",
       "  ('defensive', 29),\n",
       "  ('place', 29),\n",
       "  ('end', 29),\n",
       "  ('argument', 29),\n",
       "  ('fair', 29),\n",
       "  ('sport', 28),\n",
       "  ('completely', 28),\n",
       "  ('word', 28),\n",
       "  ('help', 28),\n",
       "  ('draymond', 28),\n",
       "  ('solid', 28),\n",
       "  ('comes', 28),\n",
       "  ('outside', 28),\n",
       "  ('sorry', 28),\n",
       "  ('gone', 28),\n",
       "  ('opinion', 28),\n",
       "  ('losing', 28),\n",
       "  ('gave', 28),\n",
       "  ('harris', 28),\n",
       "  ('non', 28),\n",
       "  ('floor', 28),\n",
       "  ('signed', 28),\n",
       "  ('title', 28),\n",
       "  ('full', 28),\n",
       "  ('must', 28),\n",
       "  ('huge', 28),\n",
       "  ('says', 28),\n",
       "  ('rondo', 28),\n",
       "  ('black', 28),\n",
       "  ('whatever', 27),\n",
       "  ('actual', 27),\n",
       "  ('thinking', 27),\n",
       "  ('center', 27),\n",
       "  ('seriously', 27),\n",
       "  ('seem', 27),\n",
       "  ('contracts', 27),\n",
       "  ('wiggins', 27),\n",
       "  ('kinda', 27),\n",
       "  ('dumb', 27),\n",
       "  ('matter', 27),\n",
       "  ('heard', 27),\n",
       "  ('ben', 27),\n",
       "  ('bring', 27),\n",
       "  ('record', 27),\n",
       "  ('shoot', 27),\n",
       "  ('happens', 27),\n",
       "  ('ta', 27),\n",
       "  ('jokic', 27),\n",
       "  ('happy', 27),\n",
       "  ('bron', 27),\n",
       "  ('home', 26),\n",
       "  ('hes', 26),\n",
       "  ('situation', 26),\n",
       "  ('smart', 26),\n",
       "  ('drafted', 26),\n",
       "  ('george', 26),\n",
       "  ('straight', 26),\n",
       "  ('respect', 26),\n",
       "  ('wade', 26),\n",
       "  ('terrible', 26),\n",
       "  ('mind', 26),\n",
       "  ('9', 26),\n",
       "  ('otherwise', 26),\n",
       "  ('role', 26),\n",
       "  ('tbh', 26),\n",
       "  ('wtf', 26),\n",
       "  ('kings', 26),\n",
       "  ('bunch', 25),\n",
       "  ('cool', 25),\n",
       "  ('white', 25),\n",
       "  ('leave', 25),\n",
       "  ('single', 25),\n",
       "  ('trading', 25),\n",
       "  ('offensive', 25),\n",
       "  ('healthy', 25),\n",
       "  ('missed', 25),\n",
       "  ('mad', 25),\n",
       "  ('left', 25),\n",
       "  ('step', 25),\n",
       "  ('city', 25),\n",
       "  ('bro', 25),\n",
       "  ('durant', 25),\n",
       "  ('shaq', 25),\n",
       "  ('takes', 25),\n",
       "  ('hand', 25),\n",
       "  ('watched', 25),\n",
       "  ('prime', 25),\n",
       "  ('sounds', 25),\n",
       "  ('wins', 25),\n",
       "  ('current', 25),\n",
       "  ('deserve', 25),\n",
       "  ('power', 24),\n",
       "  ('works', 24),\n",
       "  ('bit', 24),\n",
       "  ('important', 24),\n",
       "  ('room', 24),\n",
       "  ('due', 24),\n",
       "  ('anyway', 24),\n",
       "  ('clear', 24),\n",
       "  ('higher', 24),\n",
       "  ('danny', 24),\n",
       "  ('news', 24),\n",
       "  ('rookie', 24),\n",
       "  ('state', 24),\n",
       "  ('dlo', 24),\n",
       "  ('shots', 24),\n",
       "  ('thanks', 24),\n",
       "  ('set', 24),\n",
       "  ('stuff', 23),\n",
       "  ('seasons', 23),\n",
       "  ('low', 23),\n",
       "  ('line', 23),\n",
       "  ('conley', 23),\n",
       "  ('similar', 23),\n",
       "  ('term', 23),\n",
       "  ('serious', 23),\n",
       "  ('im', 23),\n",
       "  ('behind', 23),\n",
       "  ('months', 23),\n",
       "  ('spanish', 23),\n",
       "  ('idk', 23),\n",
       "  ('okay', 23),\n",
       "  ('suns', 23),\n",
       "  ('assets', 23),\n",
       "  ('joke', 23),\n",
       "  ('beal', 22),\n",
       "  ('coaches', 22),\n",
       "  ('anymore', 22),\n",
       "  ('russell', 22),\n",
       "  ('amount', 22),\n",
       "  ('heat', 22),\n",
       "  ('doncic', 22),\n",
       "  ('looked', 22),\n",
       "  ('ton', 22),\n",
       "  ('op', 22),\n",
       "  ('late', 22),\n",
       "  ('surprised', 22),\n",
       "  ('double', 22),\n",
       "  ('become', 22),\n",
       "  ('type', 22),\n",
       "  ('compared', 22),\n",
       "  ('pacers', 22),\n",
       "  ('haha', 22),\n",
       "  ('shooter', 22),\n",
       "  ('bust', 22),\n",
       "  ('12', 22),\n",
       "  ('c', 22),\n",
       "  ('together', 22),\n",
       "  ('stat', 22),\n",
       "  ('dame', 22),\n",
       "  ('funny', 22),\n",
       "  ('thank', 22),\n",
       "  ('foot', 22),\n",
       "  ('core', 21),\n",
       "  ('laker', 21),\n",
       "  ('tonight', 21),\n",
       "  ('paid', 21),\n",
       "  ('idea', 21),\n",
       "  ('tried', 21),\n",
       "  ('superstar', 21),\n",
       "  ('couple', 21),\n",
       "  ('anthony', 21),\n",
       "  ('teammates', 21),\n",
       "  ('forget', 21),\n",
       "  ('zubac', 21),\n",
       "  ('person', 21),\n",
       "  ('jimmy', 21),\n",
       "  ('demar', 21),\n",
       "  ('russ', 21),\n",
       "  ('hey', 21),\n",
       "  ('athletes', 20),\n",
       "  ('leaving', 20),\n",
       "  ('glad', 20),\n",
       "  ('difference', 20),\n",
       "  ('ones', 20),\n",
       "  ('l', 20),\n",
       "  ('saw', 20),\n",
       "  ('knew', 20),\n",
       "  ('hurt', 20),\n",
       "  ('asked', 20),\n",
       "  ('main', 20),\n",
       "  ('thunder', 20),\n",
       "  ('based', 20),\n",
       "  ('porzingis', 20),\n",
       "  ('throw', 20),\n",
       "  ('sit', 20),\n",
       "  ('men', 20),\n",
       "  ('obvious', 20),\n",
       "  ('dirk', 20),\n",
       "  ('kid', 20),\n",
       "  ('espn', 20),\n",
       "  ('fucked', 20),\n",
       "  ('fine', 20),\n",
       "  ('course', 20),\n",
       "  ('nola', 20),\n",
       "  ('scoring', 20),\n",
       "  ('x200b', 19),\n",
       "  ('example', 19),\n",
       "  ('interesting', 19),\n",
       "  ('video', 19),\n",
       "  ('third', 19),\n",
       "  ('except', 19),\n",
       "  ('organization', 19),\n",
       "  ('buy', 19),\n",
       "  ('0', 19),\n",
       "  ('change', 19),\n",
       "  ('unless', 19),\n",
       "  ('youtube', 19),\n",
       "  ('tough', 19),\n",
       "  ('seeing', 19),\n",
       "  ('ya', 19),\n",
       "  ('told', 19),\n",
       "  ('yea', 19),\n",
       "  ('tobias', 19),\n",
       "  ('insane', 19),\n",
       "  ('25', 19),\n",
       "  ('weird', 19),\n",
       "  ('others', 19),\n",
       "  ('boy', 19),\n",
       "  ('toronto', 19),\n",
       "  ('fa', 19),\n",
       "  ('starters', 19),\n",
       "  ('regular', 19),\n",
       "  ('philly', 19),\n",
       "  ('e', 19),\n",
       "  ('multiple', 19),\n",
       "  ('kids', 19),\n",
       "  ('moment', 19),\n",
       "  ('mj', 18),\n",
       "  ('easy', 18),\n",
       "  ('deserves', 18),\n",
       "  ('willing', 18),\n",
       "  ('telling', 18),\n",
       "  ('ncaa', 18),\n",
       "  ('injuries', 18),\n",
       "  ('hilarious', 18),\n",
       "  ('bruh', 18),\n",
       "  ('silver', 18),\n",
       "  ('ring', 18),\n",
       "  ('add', 18),\n",
       "  ('averaging', 18),\n",
       "  ('poor', 18),\n",
       "  ('match', 18),\n",
       "  ('gsw', 18),\n",
       "  ('greatest', 18),\n",
       "  ('certainly', 18),\n",
       "  ('wonder', 18),\n",
       "  ('b', 18),\n",
       "  ('stretch', 18),\n",
       "  ('issue', 18),\n",
       "  ('legit', 18),\n",
       "  ('conference', 18),\n",
       "  ('green', 18),\n",
       "  ('effort', 18),\n",
       "  ('orleans', 18),\n",
       "  ('putting', 18),\n",
       "  ('signing', 18),\n",
       "  ('head', 18),\n",
       "  ('meant', 18),\n",
       "  ('wow', 18),\n",
       "  ('fo', 18),\n",
       "  ('assists', 18),\n",
       "  ('dunk', 18),\n",
       "  ('ny', 18),\n",
       "  ('50', 18),\n",
       "  ('easily', 18),\n",
       "  ('totally', 18),\n",
       "  ('reading', 17),\n",
       "  ('form', 17),\n",
       "  ('open', 17),\n",
       "  ('nfl', 17),\n",
       "  ('bullshit', 17),\n",
       "  ('act', 17),\n",
       "  ('owners', 17),\n",
       "  ('pass', 17),\n",
       "  ('flair', 17),\n",
       "  ('wish', 17),\n",
       "  ('kemba', 17),\n",
       "  ('pistons', 17),\n",
       "  ('confirmed', 17),\n",
       "  ('2016', 17),\n",
       "  ('offered', 17),\n",
       "  ('age', 17),\n",
       "  ('ahead', 17),\n",
       "  ('hayward', 17),\n",
       "  ('ways', 17),\n",
       "  ('overall', 17),\n",
       "  ('36', 17),\n",
       "  ('five', 17),\n",
       "  ('facts', 17),\n",
       "  ('squad', 17),\n",
       "  ('ts', 17),\n",
       "  ('hawks', 17),\n",
       "  ('waiting', 17),\n",
       "  ('given', 17),\n",
       "  ('blake', 17),\n",
       "  ('reddit', 17),\n",
       "  ('rule', 17),\n",
       "  ('attention', 17),\n",
       "  ('article', 17),\n",
       "  ('fit', 17),\n",
       "  ('perfect', 17),\n",
       "  ('acting', 17),\n",
       "  ('turn', 17),\n",
       "  ('shitty', 17),\n",
       "  ('booker', 17),\n",
       "  ('favorite', 17),\n",
       "  ('elite', 17),\n",
       "  ('deadline', 16),\n",
       "  ('feels', 16),\n",
       "  ('position', 16),\n",
       "  ('using', 16),\n",
       "  ('76ers', 16),\n",
       "  ('jrue', 16),\n",
       "  ('decent', 16),\n",
       "  ('3pt', 16),\n",
       "  ('mention', 16),\n",
       "  ('school', 16),\n",
       "  ('dallas', 16),\n",
       "  ('story', 16),\n",
       "  ('week', 16),\n",
       "  ('considering', 16),\n",
       "  ('bulls', 16),\n",
       "  ('boogie', 16),\n",
       "  ('arguing', 16),\n",
       "  ('family', 16),\n",
       "  ('v', 16),\n",
       "  ('boban', 16),\n",
       "  ('smith', 16),\n",
       "  ('11', 16),\n",
       "  ('expect', 16),\n",
       "  ('certain', 16),\n",
       "  ('decision', 16),\n",
       "  ('score', 16),\n",
       "  ('running', 16),\n",
       "  ('lmfao', 16),\n",
       "  ('process', 16),\n",
       "  ('shut', 16),\n",
       "  ('lottery', 16),\n",
       "  ('lin', 16),\n",
       "  ('tank', 16),\n",
       "  ('somebody', 16),\n",
       "  ('mid', 16),\n",
       "  ('posted', 16),\n",
       "  ('success', 16),\n",
       "  ('major', 16),\n",
       "  ('siakam', 16),\n",
       "  ('buddy', 16),\n",
       "  ('miami', 16),\n",
       "  ('allowed', 15),\n",
       "  ('2019', 15),\n",
       "  ('cousins', 15),\n",
       "  ('crying', 15),\n",
       "  ('calling', 15),\n",
       "  ('biggest', 15),\n",
       "  ('hornets', 15),\n",
       "  ('pieces', 15),\n",
       "  ('possibly', 15),\n",
       "  ('rotation', 15),\n",
       "  ('3rd', 15),\n",
       "  ('suck', 15),\n",
       "  ('seconds', 15),\n",
       "  ('known', 15),\n",
       "  ('size', 15),\n",
       "  ('pop', 15),\n",
       "  ('reasons', 15),\n",
       "  ('kevin', 15),\n",
       "  ('soon', 15),\n",
       "  ('refs', 15),\n",
       "  ('agency', 15),\n",
       "  ('along', 15),\n",
       "  ('asking', 15),\n",
       "  ('impossible', 15),\n",
       "  ('disagree', 15),\n",
       "  ('trae', 15),\n",
       "  ('scenario', 15),\n",
       "  ('wan', 15),\n",
       "  ('ready', 15),\n",
       "  ('dominant', 15),\n",
       "  ('realize', 15),\n",
       "  ('middle', 15),\n",
       "  ('shows', 15),\n",
       "  ('brother', 15),\n",
       "  ('system', 15),\n",
       "  ('dwight', 15),\n",
       "  ('asset', 15),\n",
       "  ('report', 14),\n",
       "  ('somehow', 14),\n",
       "  ('rules', 14),\n",
       "  ('general', 14),\n",
       "  ('blazers', 14),\n",
       "  ('nets', 14),\n",
       "  ('forward', 14),\n",
       "  ('lo', 14),\n",
       "  ('sucks', 14),\n",
       "  ('mirotic', 14),\n",
       "  ('rim', 14),\n",
       "  ('mostly', 14),\n",
       "  ('sometimes', 14),\n",
       "  ('doesnt', 14),\n",
       "  ('snubbed', 14),\n",
       "  ('highly', 14),\n",
       "  ('rid', 14),\n",
       "  ('tanking', 14),\n",
       "  ('side', 14),\n",
       "  ('hot', 14),\n",
       "  ('raps', 14),\n",
       "  ('body', 14),\n",
       "  ('reference', 14),\n",
       "  ('usually', 14),\n",
       "  ('york', 14),\n",
       "  ('extremely', 14),\n",
       "  ('steve', 14),\n",
       "  ('alright', 14),\n",
       "  ('dsj', 14),\n",
       "  ('lineup', 14),\n",
       "  ('earlier', 14),\n",
       "  ('percentage', 14),\n",
       "  ('force', 14),\n",
       "  ('dad', 14),\n",
       "  ('ibaka', 14),\n",
       "  ('national', 14),\n",
       "  ('alone', 14),\n",
       "  ('16', 14),\n",
       "  ('18', 14),\n",
       "  ('recently', 14),\n",
       "  ('horford', 14),\n",
       "  ('awesome', 14),\n",
       "  ('ah', 14),\n",
       "  ('jesus', 14),\n",
       "  ('smh', 14),\n",
       "  ('exact', 14),\n",
       "  ('luke', 14),\n",
       "  ('often', 14),\n",
       "  ('sad', 14),\n",
       "  ('impact', 14),\n",
       "  ('horrible', 14),\n",
       "  ('13', 14),\n",
       "  ('longer', 14),\n",
       "  ('comments', 14),\n",
       "  ('fake', 14),\n",
       "  ('twitter', 14),\n",
       "  ('rudy', 14),\n",
       "  ('pau', 14),\n",
       "  ('clutch', 14),\n",
       "  ('personal', 13),\n",
       "  ('ridiculous', 13),\n",
       "  ('35', 13),\n",
       "  ('quarter', 13),\n",
       "  ('rights', 13),\n",
       "  ('4th', 13),\n",
       "  ('adams', 13),\n",
       "  ('keeps', 13),\n",
       "  ('check', 13),\n",
       "  ('efficient', 13),\n",
       "  ('baby', 13),\n",
       "  ('threads', 13),\n",
       "  ('barely', 13),\n",
       "  ('trust', 13),\n",
       "  ('19', 13),\n",
       "  ('grizzlies', 13),\n",
       "  ('wizards', 13),\n",
       "  ('essentially', 13),\n",
       "  ('pull', 13),\n",
       "  ('nearly', 13),\n",
       "  ('paying', 13),\n",
       "  ('enjoy', 13),\n",
       "  ('building', 13),\n",
       "  ('hoping', 13),\n",
       "  ('24', 13),\n",
       "  ('fully', 13),\n",
       "  ('despite', 13),\n",
       "  ('net', 13),\n",
       "  ('self', 13),\n",
       "  ('return', 13),\n",
       "  ('package', 13),\n",
       "  ('control', 13),\n",
       "  ('ability', 13),\n",
       "  ('country', 13),\n",
       "  ('2020', 13),\n",
       "  ('spain', 13),\n",
       "  ('jersey', 13),\n",
       "  ('worked', 13),\n",
       "  ('chris', 13),\n",
       "  ('knowing', 13),\n",
       "  ('offseason', 13),\n",
       "  ('short', 13),\n",
       "  ('answer', 13),\n",
       "  ('needed', 13),\n",
       "  ('join', 13),\n",
       "  ('defend', 13),\n",
       "  ('bigger', 13),\n",
       "  ('athletic', 13),\n",
       "  ('regardless', 13),\n",
       "  ('barnes', 13),\n",
       "  ('spots', 13),\n",
       "  ('mins', 13),\n",
       "  ('sound', 13),\n",
       "  ('happening', 13),\n",
       "  ('cleveland', 13),\n",
       "  ('explain', 13),\n",
       "  ('narrative', 13),\n",
       "  ('deserved', 13),\n",
       "  ('rose', 13),\n",
       "  ('easier', 13),\n",
       "  ('mistake', 13),\n",
       "  ('fault', 13),\n",
       "  ('beasley', 13),\n",
       "  ('rings', 13),\n",
       "  ('travel', 12),\n",
       "  ('shoes', 12),\n",
       "  ('conversation', 12),\n",
       "  ('speak', 12),\n",
       "  ('discussion', 12),\n",
       "  ('bs', 12),\n",
       "  ('working', 12),\n",
       "  ('kat', 12),\n",
       "  ('dennis', 12),\n",
       "  ('harder', 12),\n",
       "  ('gives', 12),\n",
       "  ('era', 12),\n",
       "  ('taken', 12),\n",
       "  ('credit', 12),\n",
       "  ('weight', 12),\n",
       "  ('agreed', 12),\n",
       "  ('within', 12),\n",
       "  ('support', 12),\n",
       "  ('break', 12),\n",
       "  ('forever', 12),\n",
       "  ('fall', 12),\n",
       "  ('expected', 12),\n",
       "  ('area', 12),\n",
       "  ('sort', 12),\n",
       "  ('shoe', 12),\n",
       "  ('order', 12),\n",
       "  ('month', 12),\n",
       "  ('management', 12),\n",
       "  ('opinions', 12),\n",
       "  ('n', 12),\n",
       "  ('jump', 12),\n",
       "  ('finally', 12),\n",
       "  ('allen', 12),\n",
       "  ('contest', 12),\n",
       "  ('johnson', 12),\n",
       "  ('hours', 12),\n",
       "  ('averaged', 12),\n",
       "  ('older', 12),\n",
       "  ('dipo', 12),\n",
       "  ('john', 12),\n",
       "  ('gms', 12),\n",
       "  ('argue', 12),\n",
       "  ('thinks', 12),\n",
       "  ('hopefully', 12),\n",
       "  ('quote', 12),\n",
       "  ('mom', 12),\n",
       "  ('context', 12),\n",
       "  ('oladipo', 12),\n",
       "  ('calls', 12),\n",
       "  ('leonard', 12),\n",
       "  ('tv', 12),\n",
       "  ('idiot', 12),\n",
       "  ('golden', 12),\n",
       "  ('hear', 12),\n",
       "  ('becomes', 12),\n",
       "  ('words', 12),\n",
       "  ('didnt', 12),\n",
       "  ('options', 12),\n",
       "  ('focus', 12),\n",
       "  ('terms', 12),\n",
       "  ('leaves', 12),\n",
       "  ('loss', 12),\n",
       "  ('deep', 12),\n",
       "  ('loyalty', 12),\n",
       "  ('majority', 12),\n",
       "  ('son', 12),\n",
       "  ('arguably', 12),\n",
       "  ('truly', 12),\n",
       "  ('posts', 12),\n",
       "  ...]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Subreddit == 'nba'].Values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's fine. We accept everyone from everywhere even outside the country. The only thing is you would have to come to our location for your dedications and the elevations after that.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group['body'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['it', 's', 'fine']),\n",
       " WordList(['s', 'fine', 'we']),\n",
       " WordList(['fine', 'we', 'accept']),\n",
       " WordList(['we', 'accept', 'everyone']),\n",
       " WordList(['accept', 'everyone', 'from']),\n",
       " WordList(['everyone', 'from', 'everywhere']),\n",
       " WordList(['from', 'everywhere', 'even']),\n",
       " WordList(['everywhere', 'even', 'outside']),\n",
       " WordList(['even', 'outside', 'the']),\n",
       " WordList(['outside', 'the', 'country']),\n",
       " WordList(['the', 'country', 'the']),\n",
       " WordList(['country', 'the', 'only']),\n",
       " WordList(['the', 'only', 'thing']),\n",
       " WordList(['only', 'thing', 'is']),\n",
       " WordList(['thing', 'is', 'you']),\n",
       " WordList(['is', 'you', 'would']),\n",
       " WordList(['you', 'would', 'have']),\n",
       " WordList(['would', 'have', 'to']),\n",
       " WordList(['have', 'to', 'come']),\n",
       " WordList(['to', 'come', 'to']),\n",
       " WordList(['come', 'to', 'our']),\n",
       " WordList(['to', 'our', 'location']),\n",
       " WordList(['our', 'location', 'for']),\n",
       " WordList(['location', 'for', 'your']),\n",
       " WordList(['for', 'your', 'dedications']),\n",
       " WordList(['your', 'dedications', 'and']),\n",
       " WordList(['dedications', 'and', 'the']),\n",
       " WordList(['and', 'the', 'elevations']),\n",
       " WordList(['the', 'elevations', 'after']),\n",
       " WordList(['elevations', 'after', 'that'])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "\n",
    "sample_text = re.sub(r'\\W+', ' ', group['body'][7].lower())\n",
    "TextBlob(sample_text).ngrams(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(self, word):\n",
    "    '''\n",
    "    Utility function to determine sentiment of passed word \n",
    "    using textblob's sentiment method\n",
    "    '''\n",
    "    \n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(self.clean(word)) \n",
    "    \n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"positive\"\n",
    "    elif analysis.sentiment.polarity ==0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-3ce3b6288d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "def sentiment_func(x):\n",
    "    sentiment = TextBlob(x)\n",
    "    x['polarity'] = sentiment.polarity\n",
    "    x['subjectivity'] = sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "sample = clean_df(sample_text).apply(sentiment_func, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER - Valence Aware Dictionary and sEntiment Reasoner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the service isn't good!----------------- {'neg': 0.473, 'neu': 0.527, 'pos': 0.0, 'compound': -0.4015}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"the service isn't good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the service isn't extremely good!------- {'neg': 0.422, 'neu': 0.578, 'pos': 0.0, 'compound': -0.4432}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"the service isn't extremely good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food here isnâ€™t really all that great {'neg': 0.0, 'neu': 0.616, 'pos': 0.384, 'compound': 0.6557}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"The food here isnâ€™t really all that great\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20103, 235119)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Generate matrix of wrd vectors (example)\n",
    "bow_matrix = vectorizer.fit_transform(clean_df['body']) \n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-d2f0ae6ef0b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_matrix_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_matrix_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1031\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    327\u001b[0m                                                tokenize)\n\u001b[1;32m    328\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 329\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "bow_matrix_2 = vectorizer.fit_transform(token_df['body']) \n",
    "print(bow_matrix_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u w u so far it goes to mk4 but i dont have t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i think that s a good idea her constant bad us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey there looks like you re not using our stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ok no problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>well deserved if my girls look that good at 40...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0   u w u so far it goes to mk4 but i dont have t...\n",
       "1  i think that s a good idea her constant bad us...\n",
       "2  hey there looks like you re not using our stan...\n",
       "3                                      ok no problem\n",
       "4  well deserved if my girls look that good at 40..."
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EmilyBloom</th>\n",
       "      <td>-62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karleegrey</th>\n",
       "      <td>-27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cumberbitches</th>\n",
       "      <td>-26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>badpsychology</th>\n",
       "      <td>-25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purrito</th>\n",
       "      <td>-22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FortniteAfterDark</th>\n",
       "      <td>-21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megturney</th>\n",
       "      <td>-17.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anriokita</th>\n",
       "      <td>-16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarvelCringe</th>\n",
       "      <td>-16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OnOffCelebs</th>\n",
       "      <td>-14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UsernameChecksOut</th>\n",
       "      <td>-13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GillianBarnes</th>\n",
       "      <td>-13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewPatriotism</th>\n",
       "      <td>-12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YanetGarcia</th>\n",
       "      <td>-12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigBlackBootyGIFS</th>\n",
       "      <td>-11.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheCuddlePuddle</th>\n",
       "      <td>-11.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARams</th>\n",
       "      <td>-10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RhodeIsland</th>\n",
       "      <td>-9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csgobetting</th>\n",
       "      <td>-8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IggyAzalea</th>\n",
       "      <td>-8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score\n",
       "subreddit                   \n",
       "EmilyBloom        -62.000000\n",
       "karleegrey        -27.000000\n",
       "Cumberbitches     -26.000000\n",
       "badpsychology     -25.000000\n",
       "Purrito           -22.000000\n",
       "FortniteAfterDark -21.000000\n",
       "Megturney         -17.666667\n",
       "anriokita         -16.500000\n",
       "MarvelCringe      -16.000000\n",
       "OnOffCelebs       -14.000000\n",
       "UsernameChecksOut -13.000000\n",
       "GillianBarnes     -13.000000\n",
       "NewPatriotism     -12.000000\n",
       "YanetGarcia       -12.000000\n",
       "BigBlackBootyGIFS -11.666667\n",
       "TheCuddlePuddle   -11.333333\n",
       "LARams            -10.000000\n",
       "RhodeIsland        -9.500000\n",
       "csgobetting        -8.000000\n",
       "IggyAzalea         -8.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data.groupby('subreddit')['score'].agg('mean').sort_values().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KamikazeByWords</th>\n",
       "      <td>266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>menwritingwomen</th>\n",
       "      <td>174.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheDepthsBelow</th>\n",
       "      <td>162.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preyingmantis</th>\n",
       "      <td>140.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likeus</th>\n",
       "      <td>122.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arresteddevelopment</th>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u_RedditCancerBot420</th>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watchplantsgrow</th>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BreadStapledToTrees</th>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u_windowsdev_team</th>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AfterTheLoop</th>\n",
       "      <td>83.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oldschoolcreepy</th>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheMonkeysPaw</th>\n",
       "      <td>74.637209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WatchPeopleDieInside</th>\n",
       "      <td>73.622047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDOWORKHERELADY</th>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curb</th>\n",
       "      <td>72.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instantkarma</th>\n",
       "      <td>71.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuckHOA</th>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slammywhammies</th>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youdontsurf</th>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "subreddit                       \n",
       "KamikazeByWords       266.000000\n",
       "menwritingwomen       174.750000\n",
       "TheDepthsBelow        162.800000\n",
       "preyingmantis         140.666667\n",
       "likeus                122.666667\n",
       "arresteddevelopment   111.000000\n",
       "u_RedditCancerBot420  101.000000\n",
       "watchplantsgrow        98.000000\n",
       "BreadStapledToTrees    85.000000\n",
       "u_windowsdev_team      85.000000\n",
       "AfterTheLoop           83.500000\n",
       "oldschoolcreepy        75.000000\n",
       "TheMonkeysPaw          74.637209\n",
       "WatchPeopleDieInside   73.622047\n",
       "IDOWORKHERELADY        73.000000\n",
       "curb                   72.500000\n",
       "instantkarma           71.914286\n",
       "fuckHOA                71.000000\n",
       "slammywhammies         69.000000\n",
       "youdontsurf            68.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data.groupby('subreddit')['score'].agg('mean').sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "#     print(\"{}\".format(str(score)))\n",
    "    return score\n",
    "top_20 = data.sort_values('subreddit').reset_index()\n",
    "top_20['senti']= data['body'].map(lambda t: sentiment_analyzer_scores(str(t))) \n",
    "top_20['senti_neu'] = top_20.senti.apply(lambda x: x.get('neu'))\n",
    "top_20['senti_pos'] = top_20.senti.apply(lambda x: x.get('pos'))\n",
    "top_20['senti_neg'] = top_20.senti.apply(lambda x: x.get('neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Callieism</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VTCC</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProdoGlassHeads</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exteenagers</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MHOCHolyrood</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLMMJDISPENSARIES</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GoldReplies</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babybigcatgifs</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HungryButts</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabikaeru</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PokemonDraftLeagues</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newreddits_nsfw</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joshua</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LondonSpitfire</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweetfru1t</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fisting</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FishingOntario</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FishingAustralia</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euro4euro</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     senti_pos\n",
       "subreddit                     \n",
       "Callieism                  1.0\n",
       "VTCC                       1.0\n",
       "ProdoGlassHeads            1.0\n",
       "exteenagers                1.0\n",
       "MHOCHolyrood               1.0\n",
       "FLMMJDISPENSARIES          1.0\n",
       "GoldReplies                1.0\n",
       "babybigcatgifs             1.0\n",
       "HungryButts                1.0\n",
       "tabikaeru                  1.0\n",
       "PokemonDraftLeagues        1.0\n",
       "newreddits_nsfw            1.0\n",
       "Joshua                     1.0\n",
       "LondonSpitfire             1.0\n",
       "sweetfru1t                 1.0\n",
       "Fisting                    1.0\n",
       "FishingOntario             1.0\n",
       "FishingAustralia           1.0\n",
       "euro4euro                  1.0\n",
       "ISTJ                       1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(top_20.groupby('subreddit')['senti_pos'].agg('mean').sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sciencestudioyt</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>republicans</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rprogramming</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rarebooks</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>castration</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BattleForTheGrid</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>celebritylookalike</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audacity</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leahgotti</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oldmaps</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lmGoingToHellForThis</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>djiphantom</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R4OlderWomen</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anet3DPrinters</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pocketwatch</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hankeystoys</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ice_Poseidon_CC</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edinburgh_University</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sabrinalynnci</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarylandPolitics</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nakedgirlsdancing</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daggerfallunity</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheFrights</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FullXxXHD</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>javdreams</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FuzzySwagArmy</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The_Pro14</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dndmaps</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PokemonSwordShield</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PokemonGoMetroDetroit</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StupidAssMemes</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoliticalHumour</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoLimitsCoaster</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GakkouGurashi</th>\n",
       "      <td>0.8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emovinyl</th>\n",
       "      <td>0.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnomaliesUnleashed</th>\n",
       "      <td>0.8780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmrprogram</th>\n",
       "      <td>0.8720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ancientgreece</th>\n",
       "      <td>0.8610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LexiBelle</th>\n",
       "      <td>0.8310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seethru</th>\n",
       "      <td>0.8310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rundisney</th>\n",
       "      <td>0.8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MicroPorn</th>\n",
       "      <td>0.8210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autoimmunity</th>\n",
       "      <td>0.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>runes</th>\n",
       "      <td>0.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConnectWise</th>\n",
       "      <td>0.8080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cornouija</th>\n",
       "      <td>0.8060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PrayersToTrump</th>\n",
       "      <td>0.7960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geedis</th>\n",
       "      <td>0.7920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classicmods</th>\n",
       "      <td>0.7905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       senti_neg\n",
       "subreddit                       \n",
       "sciencestudioyt           1.0000\n",
       "republicans               1.0000\n",
       "rprogramming              1.0000\n",
       "rarebooks                 1.0000\n",
       "castration                1.0000\n",
       "BattleForTheGrid          1.0000\n",
       "celebritylookalike        1.0000\n",
       "audacity                  1.0000\n",
       "leahgotti                 1.0000\n",
       "oldmaps                   1.0000\n",
       "lmGoingToHellForThis      1.0000\n",
       "djiphantom                1.0000\n",
       "R4OlderWomen              1.0000\n",
       "Anet3DPrinters            1.0000\n",
       "diet                      1.0000\n",
       "pocketwatch               1.0000\n",
       "Hankeystoys               1.0000\n",
       "Ice_Poseidon_CC           1.0000\n",
       "Edinburgh_University      1.0000\n",
       "sabrinalynnci             1.0000\n",
       "MarylandPolitics          1.0000\n",
       "nakedgirlsdancing         1.0000\n",
       "daggerfallunity           1.0000\n",
       "TheFrights                1.0000\n",
       "FullXxXHD                 1.0000\n",
       "javdreams                 1.0000\n",
       "FuzzySwagArmy             1.0000\n",
       "The_Pro14                 1.0000\n",
       "dndmaps                   1.0000\n",
       "PokemonSwordShield        1.0000\n",
       "PokemonGoMetroDetroit     1.0000\n",
       "StupidAssMemes            1.0000\n",
       "PoliticalHumour           1.0000\n",
       "NoLimitsCoaster           1.0000\n",
       "GakkouGurashi             0.8890\n",
       "Emovinyl                  0.8880\n",
       "AnomaliesUnleashed        0.8780\n",
       "hmrprogram                0.8720\n",
       "ancientgreece             0.8610\n",
       "LexiBelle                 0.8310\n",
       "seethru                   0.8310\n",
       "rundisney                 0.8300\n",
       "MicroPorn                 0.8210\n",
       "autoimmunity              0.8150\n",
       "runes                     0.8150\n",
       "ConnectWise               0.8080\n",
       "cornouija                 0.8060\n",
       "PrayersToTrump            0.7960\n",
       "Geedis                    0.7920\n",
       "classicmods               0.7905"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(top_20.groupby('subreddit')['senti_neg'].agg('mean').sort_values(ascending=False).head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti_neu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zztails</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bookporn</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>budgetwise</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bsv</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bspwm</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brokenthoughts</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brissabreezy</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brazilianjiujitsu</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bravelydefault</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>braandpanties</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bprogramming</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boxingstar</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bostoncollege</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bored</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>borderlands3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bookmarklets</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burial</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bookcoverporn</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonnarootickets</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bomberman</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   senti_neu\n",
       "subreddit                   \n",
       "zztails                  1.0\n",
       "bookporn                 1.0\n",
       "budgetwise               1.0\n",
       "bsv                      1.0\n",
       "bspwm                    1.0\n",
       "brokenthoughts           1.0\n",
       "brissabreezy             1.0\n",
       "brazilianjiujitsu        1.0\n",
       "bravelydefault           1.0\n",
       "braandpanties            1.0\n",
       "bprogramming             1.0\n",
       "boxingstar               1.0\n",
       "bostoncollege            1.0\n",
       "bored                    1.0\n",
       "borderlands3             1.0\n",
       "bookmarklets             1.0\n",
       "burial                   1.0\n",
       "bookcoverporn            1.0\n",
       "bonnarootickets          1.0\n",
       "bomberman                1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(top_20.groupby('subreddit')['senti_neu'].agg('mean').sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>senti_neg</th>\n",
       "      <th>senti_neu</th>\n",
       "      <th>senti_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>374661</th>\n",
       "      <td>mildlyinfuriating</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50460</th>\n",
       "      <td>AskWomen</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22863</th>\n",
       "      <td>AskOuija</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477018</th>\n",
       "      <td>u_hotelsworldwide</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31969</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403553</th>\n",
       "      <td>opieandanthony</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293909</th>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62019</th>\n",
       "      <td>BravoRealHousewives</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228631</th>\n",
       "      <td>Tinder</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407672</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56132</th>\n",
       "      <td>BigBrother</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126591</th>\n",
       "      <td>HumansBeingBros</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287537</th>\n",
       "      <td>crossdressing</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39159</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425092</th>\n",
       "      <td>pornfree</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479372</th>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237775</th>\n",
       "      <td>WTF</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183992</th>\n",
       "      <td>PoliticalHumor</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202656</th>\n",
       "      <td>SeattleWA</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273084</th>\n",
       "      <td>brasil</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  subreddit  senti_neg  senti_neu  senti_pos\n",
       "374661    mildlyinfuriating      0.072      0.868      0.060\n",
       "50460              AskWomen      0.066      0.797      0.137\n",
       "22863              AskOuija      0.074      0.858      0.068\n",
       "477018    u_hotelsworldwide      0.000      1.000      0.000\n",
       "31969             AskReddit      0.114      0.549      0.338\n",
       "403553       opieandanthony      0.000      1.000      0.000\n",
       "293909      dataisbeautiful      0.000      0.813      0.187\n",
       "62019   BravoRealHousewives      0.000      1.000      0.000\n",
       "228631               Tinder      0.000      0.896      0.104\n",
       "407672         pcmasterrace      0.108      0.829      0.063\n",
       "56132            BigBrother      0.000      0.930      0.070\n",
       "126591      HumansBeingBros      0.032      0.794      0.174\n",
       "287537        crossdressing      0.090      0.777      0.133\n",
       "39159             AskReddit      0.000      0.762      0.238\n",
       "425092             pornfree      0.072      0.928      0.000\n",
       "479372     unpopularopinion      0.000      1.000      0.000\n",
       "237775                  WTF      0.000      0.863      0.137\n",
       "183992       PoliticalHumor      0.000      1.000      0.000\n",
       "202656            SeattleWA      0.091      0.863      0.046\n",
       "273084               brasil      0.391      0.609      0.000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(top_20[['subreddit','senti_neg', 'senti_neu', 'senti_pos']]).sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
